{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"di : pythonic dependency injection di is a modern dependency injection system, modeled around the simplicity of FastAPI's dependency injection. Key features: Intuitive : simple API, inspired by FastAPI . Succinct : declare what you want, and di figures out how to assemble it using type annotations. Scopes : inspired by pytest scopes , but defined by users (no fixed \"request\" or \"session\" scopes). Customizable : decoupled internal APIs give you the flexibility to customize wiring and execution. Performant : di can execute dependencies in parallel, move sync dependencies to threads and cache results. Performance critical parts are written in \ud83e\udd80 via graphlib2 . Installation pip install di \u26a0\ufe0f This project is a work in progress. Until there is 1.X.Y release, expect breaking changes. \u26a0\ufe0f Simple Example Here is a simple example of how di works: from dataclasses import dataclass from di import Container , Dependant , SyncExecutor class A : ... class B : ... @dataclass class C : a : A b : B def main (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( C , scope = \"request\" )) with container . enter_scope ( \"request\" ): c = container . execute_sync ( solved , executor = SyncExecutor ()) assert isinstance ( c , C ) assert isinstance ( c . a , A ) assert isinstance ( c . b , B ) For more examples, see our docs . Why do I need dependency injection in Python? Isn't that a Java thing? Dependency injection is a software architecture technique that helps us achieve inversion of control and dependency inversion (one of the five SOLID design principles). It is a common misconception that traditional software design principles do not apply to Python. As a matter of fact, you are probably using a lot of these techniques already! For example, the transport argument to httpx's Client ( docs ) is an excellent example of dependency injection. Pytest, arguably the most popular Python test framework, uses dependency injection in the form of pytest fixtures . Most web frameworks employ inversion of control: when you define a view / controller, the web framework calls you! The same thing applies to CLIs (like click ) or TUIs (like Textual ). This is especially true for many newer web frameworks that not only use inversion of control but also dependency injection. Two great examples of this are FastAPI and BlackSheep . For a more comprehensive overview of Python projects related to dependency injection, see Awesome Dependency Injection in Python . Project Aims This project aims to be a general dependency injection system, with a focus on providing the underlying dependency injection functionality for other libraries. In other words, while you could use this as a standalone dependency injection framework, you may find it to be a bit terse and verbose. There are also much more mature standalone dependency injection frameworks; I would recommend at least looking into python-dependency-injector since it is currently the most popular / widely used of the bunch. For more background, see our docs .","title":"Intro"},{"location":"#di-pythonic-dependency-injection","text":"di is a modern dependency injection system, modeled around the simplicity of FastAPI's dependency injection. Key features: Intuitive : simple API, inspired by FastAPI . Succinct : declare what you want, and di figures out how to assemble it using type annotations. Scopes : inspired by pytest scopes , but defined by users (no fixed \"request\" or \"session\" scopes). Customizable : decoupled internal APIs give you the flexibility to customize wiring and execution. Performant : di can execute dependencies in parallel, move sync dependencies to threads and cache results. Performance critical parts are written in \ud83e\udd80 via graphlib2 .","title":"di: pythonic dependency injection"},{"location":"#installation","text":"pip install di \u26a0\ufe0f This project is a work in progress. Until there is 1.X.Y release, expect breaking changes. \u26a0\ufe0f","title":"Installation"},{"location":"#simple-example","text":"Here is a simple example of how di works: from dataclasses import dataclass from di import Container , Dependant , SyncExecutor class A : ... class B : ... @dataclass class C : a : A b : B def main (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( C , scope = \"request\" )) with container . enter_scope ( \"request\" ): c = container . execute_sync ( solved , executor = SyncExecutor ()) assert isinstance ( c , C ) assert isinstance ( c . a , A ) assert isinstance ( c . b , B ) For more examples, see our docs .","title":"Simple Example"},{"location":"#why-do-i-need-dependency-injection-in-python-isnt-that-a-java-thing","text":"Dependency injection is a software architecture technique that helps us achieve inversion of control and dependency inversion (one of the five SOLID design principles). It is a common misconception that traditional software design principles do not apply to Python. As a matter of fact, you are probably using a lot of these techniques already! For example, the transport argument to httpx's Client ( docs ) is an excellent example of dependency injection. Pytest, arguably the most popular Python test framework, uses dependency injection in the form of pytest fixtures . Most web frameworks employ inversion of control: when you define a view / controller, the web framework calls you! The same thing applies to CLIs (like click ) or TUIs (like Textual ). This is especially true for many newer web frameworks that not only use inversion of control but also dependency injection. Two great examples of this are FastAPI and BlackSheep . For a more comprehensive overview of Python projects related to dependency injection, see Awesome Dependency Injection in Python .","title":"Why do I need dependency injection in Python? Isn't that a Java thing?"},{"location":"#project-aims","text":"This project aims to be a general dependency injection system, with a focus on providing the underlying dependency injection functionality for other libraries. In other words, while you could use this as a standalone dependency injection framework, you may find it to be a bit terse and verbose. There are also much more mature standalone dependency injection frameworks; I would recommend at least looking into python-dependency-injector since it is currently the most popular / widely used of the bunch. For more background, see our docs .","title":"Project Aims"},{"location":"architecture/","text":"Architecture The fundamental design principle of di is to split up the complexity of dependency injection into smaller component parts: Wiring: when we discover the dependencies. This includes doing reflection (inspecting signatures), looking for dependency markers, etc. Solving: when we build an execution plan, taking into account cached values, binds, etc. Execution: when we execute dependencies, possibly doing IO, parallelization, etc. We map these responsibilities to well-defined classes/interfaces: Wiring: this is handled by Dependant Solving: this is handled by Container Execution: this is handled by Executor s There are also some auxiliary support classes: SolvedDependant holds the result of a call to Container.solve that can be passed to Container.execute_sync or Container.exeucte_async . Fundamentally, our class diagram looks like this: Mermaid diagram source classDiagram SolvedDependant \"1..n\" --o Dependant: aggregates into a DAG Container --> Dependant: visits sub-dependencies Container --> Executor: delegates execution Container --> SolvedDependant: stores solved DAG Container --> SolvedDependant: executes solved DAG class Dependant{ +get_dependencies() list~Dependant~ +register_parameter() Dependant } class SolvedDependant{ +dag Mapping~Dependant, SetOfDependant~ } class Executor{ +execute() } class Container{ +register(Provider, Dependency) +enter_scope(Scope) Container +solve(Dependant) SolvedDependant +execute(SolvedDependant, Executor) Result }","title":"Architecture"},{"location":"architecture/#architecture","text":"The fundamental design principle of di is to split up the complexity of dependency injection into smaller component parts: Wiring: when we discover the dependencies. This includes doing reflection (inspecting signatures), looking for dependency markers, etc. Solving: when we build an execution plan, taking into account cached values, binds, etc. Execution: when we execute dependencies, possibly doing IO, parallelization, etc. We map these responsibilities to well-defined classes/interfaces: Wiring: this is handled by Dependant Solving: this is handled by Container Execution: this is handled by Executor s There are also some auxiliary support classes: SolvedDependant holds the result of a call to Container.solve that can be passed to Container.execute_sync or Container.exeucte_async . Fundamentally, our class diagram looks like this: Mermaid diagram source classDiagram SolvedDependant \"1..n\" --o Dependant: aggregates into a DAG Container --> Dependant: visits sub-dependencies Container --> Executor: delegates execution Container --> SolvedDependant: stores solved DAG Container --> SolvedDependant: executes solved DAG class Dependant{ +get_dependencies() list~Dependant~ +register_parameter() Dependant } class SolvedDependant{ +dag Mapping~Dependant, SetOfDependant~ } class Executor{ +execute() } class Container{ +register(Provider, Dependency) +enter_scope(Scope) Container +solve(Dependant) SolvedDependant +execute(SolvedDependant, Executor) Result }","title":"Architecture"},{"location":"binds/","text":"Registration and Binding Provider registration serves two important functions: A way to tell the container how to assemble things that can't be auto-wired, for example interfaces. A way to override dependencies in tests. We call a the result of registering a provider a bind . Every bind in di consists of: A target callable: this can be a function, an interface / protocol or a concrete class A substitute dependency: an object implementing the DependantBase , usually just an instance of Dependant This means that binds are themselves dependencies: import sys from dataclasses import dataclass if sys . version_info < ( 3 , 8 ): from typing_extensions import Protocol else : from typing import Protocol from di import AsyncExecutor , Container , Dependant class DBProtocol ( Protocol ): async def execute ( self , sql : str ) -> None : ... async def controller ( db : DBProtocol ) -> None : await db . execute ( \"SELECT *\" ) @dataclass class DBConfig : host : str = \"localhost\" class Postgres ( DBProtocol ): def __init__ ( self , config : DBConfig ) -> None : self . host = config . host async def execute ( self , sql : str ) -> None : print ( sql ) async def framework () -> None : container = Container ( scopes = ( \"request\" ,)) container . register_by_type ( Dependant ( Postgres , scope = \"request\" ), DBProtocol ) solved = container . solve ( Dependant ( controller , scope = \"request\" )) # this next line would fail without the bind async with container . enter_scope ( \"request\" ): await container . execute_async ( solved , executor = AsyncExecutor ()) # and we can double check that the bind worked # by requesting the instance directly async with container . enter_scope ( \"request\" ): db = await container . execute_async ( container . solve ( Dependant ( DBProtocol )), executor = AsyncExecutor () ) assert isinstance ( db , Postgres ) In this example we register the Postgres class to DBProtocol , and we can see that di auto-wires Postgres as well! Registration can be used as a direct function call, in which case they are permanent, or as a context manager.","title":"Registration and Binding"},{"location":"binds/#registration-and-binding","text":"Provider registration serves two important functions: A way to tell the container how to assemble things that can't be auto-wired, for example interfaces. A way to override dependencies in tests. We call a the result of registering a provider a bind . Every bind in di consists of: A target callable: this can be a function, an interface / protocol or a concrete class A substitute dependency: an object implementing the DependantBase , usually just an instance of Dependant This means that binds are themselves dependencies: import sys from dataclasses import dataclass if sys . version_info < ( 3 , 8 ): from typing_extensions import Protocol else : from typing import Protocol from di import AsyncExecutor , Container , Dependant class DBProtocol ( Protocol ): async def execute ( self , sql : str ) -> None : ... async def controller ( db : DBProtocol ) -> None : await db . execute ( \"SELECT *\" ) @dataclass class DBConfig : host : str = \"localhost\" class Postgres ( DBProtocol ): def __init__ ( self , config : DBConfig ) -> None : self . host = config . host async def execute ( self , sql : str ) -> None : print ( sql ) async def framework () -> None : container = Container ( scopes = ( \"request\" ,)) container . register_by_type ( Dependant ( Postgres , scope = \"request\" ), DBProtocol ) solved = container . solve ( Dependant ( controller , scope = \"request\" )) # this next line would fail without the bind async with container . enter_scope ( \"request\" ): await container . execute_async ( solved , executor = AsyncExecutor ()) # and we can double check that the bind worked # by requesting the instance directly async with container . enter_scope ( \"request\" ): db = await container . execute_async ( container . solve ( Dependant ( DBProtocol )), executor = AsyncExecutor () ) assert isinstance ( db , Postgres ) In this example we register the Postgres class to DBProtocol , and we can see that di auto-wires Postgres as well! Registration can be used as a direct function call, in which case they are permanent, or as a context manager.","title":"Registration and Binding"},{"location":"cache/","text":"Dependency Cache Often, you will have dependencies that share a sub dependency. For example, you probably only want to load your configuration from environment variables once and then re-use the same object in multiple dependencies. To avoid re-computing the shared dependency, di will cache shared dependencies. How caching works Dependencies are cached by their cache key, computed in Dependant.cache_key . See dependants for more information on Dependant.cache_key . Dependencies are cached by default, but this behavior can be changed on a per-dependency basis using the use_cache=False parameter. from random import random from di import Container , Dependant , SyncExecutor from di.typing import Annotated def controller ( # no marker is equivalent to Dependant(object) v1 : object , # the default value is use_cache=True v2 : Annotated [ object , Dependant ( object , scope = \"request\" )], # but you can set use_cache=False v3 : Annotated [ float , Dependant ( random , use_cache = False , scope = \"request\" )], ) -> None : assert v1 is v2 assert v1 is not v3 and v2 is not v3 def main () -> None : container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) with container . enter_scope ( \"request\" ): container . execute_sync ( solved , executor = SyncExecutor ()) Caching and scopes Dependencies are cached within their scope and any inner scopes. Once a dependency's scope exits, it's cached value is discarded and the next time the scope is entered a fresh value will be computed.","title":"Caching"},{"location":"cache/#dependency-cache","text":"Often, you will have dependencies that share a sub dependency. For example, you probably only want to load your configuration from environment variables once and then re-use the same object in multiple dependencies. To avoid re-computing the shared dependency, di will cache shared dependencies.","title":"Dependency Cache"},{"location":"cache/#how-caching-works","text":"Dependencies are cached by their cache key, computed in Dependant.cache_key . See dependants for more information on Dependant.cache_key . Dependencies are cached by default, but this behavior can be changed on a per-dependency basis using the use_cache=False parameter. from random import random from di import Container , Dependant , SyncExecutor from di.typing import Annotated def controller ( # no marker is equivalent to Dependant(object) v1 : object , # the default value is use_cache=True v2 : Annotated [ object , Dependant ( object , scope = \"request\" )], # but you can set use_cache=False v3 : Annotated [ float , Dependant ( random , use_cache = False , scope = \"request\" )], ) -> None : assert v1 is v2 assert v1 is not v3 and v2 is not v3 def main () -> None : container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) with container . enter_scope ( \"request\" ): container . execute_sync ( solved , executor = SyncExecutor ())","title":"How caching works"},{"location":"cache/#caching-and-scopes","text":"Dependencies are cached within their scope and any inner scopes. Once a dependency's scope exits, it's cached value is discarded and the next time the scope is entered a fresh value will be computed.","title":"Caching and scopes"},{"location":"contributing/","text":"Developer setup This is a pure Python project and should be straightforward to set up on Linux or MacOS. We do not support Windows for development, if you use Windows you'll have to use VSCode DevContainers or a similar solution. We use Poetry for dependency management, and most of the config is the pyproject.toml . Linting is done via git hooks, managed by pre-commit . The linters may change over time, but they are configured in our pre-commit-config.yaml . Most of the setup and interaction with these systems is encapsulated in our Makefile . Project setup First, fork the repo and then clone your fork: $ git clone https://github.com/adriangb/di.git ---> 100% $ cd di Now install the project dependencies. You will need Make installed along with a compatible Python version (currently, 3.9.X). To set up the project, simply run: $ make init This will create a .venv virtualenv that you can configure your IDE to use. Running tests $ make test Tests are run with pytest, so you can also run them manually or configure your IDE to run them. The tests are stored in the tests/ directory. Running linting Linting will run automatically on every commit. To disable this, you can commit with git commit --no-verify . You can also run linting manually: $ make lint Documentation The docs are written as markdown and built with MkDocs. Both the docs and their source code are stored in the docs/ directory. To preview the docs locally as you edit them, run $ make docs-serve All the code fragments in the docs are stored as .py files in docs/src . These code fragments are tested as part of unit tests to ensure that the documentation stays up to date with the API. Releases This project uses continuous integration and continuous delivery on a trunk based workflow. Every merge into main should be fully functional code in a releasable state. As part of your pull request, you should propose what type of change is being made and determine the right version bump appropriately. While conventional commits are appreciated as a means of communication, especially for the merge commit, they are not required or enforced. You are however required to bump the package version in pyproject.toml . Every commit into main needs a version bump so that a release can be made, even if it is a refactor or \"chore\" type change. Once your change is merged, the new docs and PyPi package will be released automatically. Every time a release is made on PyPi, a corresponding GitHub release will be created to correlate PyPi versions to git commits.","title":"Contributing"},{"location":"contributing/#developer-setup","text":"This is a pure Python project and should be straightforward to set up on Linux or MacOS. We do not support Windows for development, if you use Windows you'll have to use VSCode DevContainers or a similar solution. We use Poetry for dependency management, and most of the config is the pyproject.toml . Linting is done via git hooks, managed by pre-commit . The linters may change over time, but they are configured in our pre-commit-config.yaml . Most of the setup and interaction with these systems is encapsulated in our Makefile .","title":"Developer setup"},{"location":"contributing/#project-setup","text":"First, fork the repo and then clone your fork: $ git clone https://github.com/adriangb/di.git ---> 100% $ cd di Now install the project dependencies. You will need Make installed along with a compatible Python version (currently, 3.9.X). To set up the project, simply run: $ make init This will create a .venv virtualenv that you can configure your IDE to use.","title":"Project setup"},{"location":"contributing/#running-tests","text":"$ make test Tests are run with pytest, so you can also run them manually or configure your IDE to run them. The tests are stored in the tests/ directory.","title":"Running tests"},{"location":"contributing/#running-linting","text":"Linting will run automatically on every commit. To disable this, you can commit with git commit --no-verify . You can also run linting manually: $ make lint","title":"Running linting"},{"location":"contributing/#documentation","text":"The docs are written as markdown and built with MkDocs. Both the docs and their source code are stored in the docs/ directory. To preview the docs locally as you edit them, run $ make docs-serve All the code fragments in the docs are stored as .py files in docs/src . These code fragments are tested as part of unit tests to ensure that the documentation stays up to date with the API.","title":"Documentation"},{"location":"contributing/#releases","text":"This project uses continuous integration and continuous delivery on a trunk based workflow. Every merge into main should be fully functional code in a releasable state. As part of your pull request, you should propose what type of change is being made and determine the right version bump appropriately. While conventional commits are appreciated as a means of communication, especially for the merge commit, they are not required or enforced. You are however required to bump the package version in pyproject.toml . Every commit into main needs a version bump so that a release can be made, even if it is a refactor or \"chore\" type change. Once your change is merged, the new docs and PyPi package will be released automatically. Every time a release is made on PyPi, a corresponding GitHub release will be created to correlate PyPi versions to git commits.","title":"Releases"},{"location":"dependants/","text":"Dependants and the DependantBase Most of these docs use Dependant as the main marker for dependencies. But the container doesn't actually know about either of these two things! In fact, the container only knows about the DependantBase , which you can find in di.api.dependencies . Dependant is just one possible implementation of the DependantBase . You can easily build your own version of Dependant by inheriting from Dependant or DependantBase . Here is an example that extracts headers from requests: from __future__ import annotations import inspect from typing import Any , Mapping , Optional , TypeVar from di import AsyncExecutor , Container , Dependant from di.typing import Annotated class Request : def __init__ ( self , headers : Mapping [ str , str ]) -> None : self . headers = { k . lower (): v for k , v in headers . items ()} class Header ( Dependant [ Any ]): def __init__ ( self , alias : Optional [ str ]) -> None : self . alias = alias super () . __init__ ( call = None , scope = \"request\" , use_cache = False ) def register_parameter ( self , param : inspect . Parameter ) -> Header : if self . alias is not None : name = self . alias else : name = param . name . replace ( \"_\" , \"-\" ) def get_header ( request : Annotated [ Request , Dependant ()]) -> str : return param . annotation ( request . headers [ name ]) self . call = get_header # We could return a copy here to allow the same Dependant # to be used in multiple places like # dep = HeaderDependant(...) # def func1(abcd = dep): ... # def func2(efgh = dep): ... # In this scenario, `dep` would be modified in func2 to set # the header name to \"efgh\", which leads to incorrect results in func1 # The solution is to return a copy here instead of self, so that # the original instance is never modified in place return self T = TypeVar ( \"T\" ) FromHeader = Annotated [ T , Header ( alias = None )] async def web_framework () -> None : container = Container ( scopes = [ \"request\" ]) valid_request = Request ( headers = { \"x-header-one\" : \"one\" , \"x-header-two\" : \"2\" }) with container . register_by_type ( Dependant ( lambda : valid_request , scope = \"request\" ), Request ): solved = container . solve ( Dependant ( controller , scope = \"request\" )) with container . enter_scope ( \"request\" ): await container . execute_async ( solved , executor = AsyncExecutor ()) # success invalid_request = Request ( headers = { \"x-header-one\" : \"one\" }) with container . register_by_type ( Dependant ( lambda : invalid_request , scope = \"request\" ), Request ): solved = container . solve ( Dependant ( controller , scope = \"request\" )) with container . enter_scope ( \"request\" ): try : await container . execute_async ( solved , executor = AsyncExecutor ()) # fails except KeyError : pass else : raise AssertionError ( \"This call should have failed because x-header-two is missing\" ) def controller ( x_header_one : FromHeader [ str ], header_two_val : Annotated [ int , Header ( alias = \"x-header-two\" )], ) -> None : \"\"\"This is the only piece of user code\"\"\" assert x_header_one == \"one\" assert header_two_val == 2 Another good example of the flexibility provided by DependantBase is the implementation of JointDependant , which lets you schedule and execute dependencies together even if they are not directly connected by wiring: from di import Container , Dependant , JoinedDependant , SyncExecutor class A : ... class B : executed = False def __init__ ( self ) -> None : B . executed = True def main (): container = Container ( scopes = ( \"request\" ,)) dependant = JoinedDependant ( Dependant ( A , scope = \"request\" ), siblings = [ Dependant ( B , scope = \"request\" )], ) solved = container . solve ( dependant ) with container . enter_scope ( \"request\" ): a = container . execute_sync ( solved , executor = SyncExecutor ()) assert isinstance ( a , A ) assert B . executed Here B is executed even though A does not depend on it. This is because JoinedDependant leverages the DependantBase interface to tell di that B is a dependency of A even if B is not a parameter or otherwise related to A .","title":"Dependants"},{"location":"dependants/#dependants-and-the-dependantbase","text":"Most of these docs use Dependant as the main marker for dependencies. But the container doesn't actually know about either of these two things! In fact, the container only knows about the DependantBase , which you can find in di.api.dependencies . Dependant is just one possible implementation of the DependantBase . You can easily build your own version of Dependant by inheriting from Dependant or DependantBase . Here is an example that extracts headers from requests: from __future__ import annotations import inspect from typing import Any , Mapping , Optional , TypeVar from di import AsyncExecutor , Container , Dependant from di.typing import Annotated class Request : def __init__ ( self , headers : Mapping [ str , str ]) -> None : self . headers = { k . lower (): v for k , v in headers . items ()} class Header ( Dependant [ Any ]): def __init__ ( self , alias : Optional [ str ]) -> None : self . alias = alias super () . __init__ ( call = None , scope = \"request\" , use_cache = False ) def register_parameter ( self , param : inspect . Parameter ) -> Header : if self . alias is not None : name = self . alias else : name = param . name . replace ( \"_\" , \"-\" ) def get_header ( request : Annotated [ Request , Dependant ()]) -> str : return param . annotation ( request . headers [ name ]) self . call = get_header # We could return a copy here to allow the same Dependant # to be used in multiple places like # dep = HeaderDependant(...) # def func1(abcd = dep): ... # def func2(efgh = dep): ... # In this scenario, `dep` would be modified in func2 to set # the header name to \"efgh\", which leads to incorrect results in func1 # The solution is to return a copy here instead of self, so that # the original instance is never modified in place return self T = TypeVar ( \"T\" ) FromHeader = Annotated [ T , Header ( alias = None )] async def web_framework () -> None : container = Container ( scopes = [ \"request\" ]) valid_request = Request ( headers = { \"x-header-one\" : \"one\" , \"x-header-two\" : \"2\" }) with container . register_by_type ( Dependant ( lambda : valid_request , scope = \"request\" ), Request ): solved = container . solve ( Dependant ( controller , scope = \"request\" )) with container . enter_scope ( \"request\" ): await container . execute_async ( solved , executor = AsyncExecutor ()) # success invalid_request = Request ( headers = { \"x-header-one\" : \"one\" }) with container . register_by_type ( Dependant ( lambda : invalid_request , scope = \"request\" ), Request ): solved = container . solve ( Dependant ( controller , scope = \"request\" )) with container . enter_scope ( \"request\" ): try : await container . execute_async ( solved , executor = AsyncExecutor ()) # fails except KeyError : pass else : raise AssertionError ( \"This call should have failed because x-header-two is missing\" ) def controller ( x_header_one : FromHeader [ str ], header_two_val : Annotated [ int , Header ( alias = \"x-header-two\" )], ) -> None : \"\"\"This is the only piece of user code\"\"\" assert x_header_one == \"one\" assert header_two_val == 2 Another good example of the flexibility provided by DependantBase is the implementation of JointDependant , which lets you schedule and execute dependencies together even if they are not directly connected by wiring: from di import Container , Dependant , JoinedDependant , SyncExecutor class A : ... class B : executed = False def __init__ ( self ) -> None : B . executed = True def main (): container = Container ( scopes = ( \"request\" ,)) dependant = JoinedDependant ( Dependant ( A , scope = \"request\" ), siblings = [ Dependant ( B , scope = \"request\" )], ) solved = container . solve ( dependant ) with container . enter_scope ( \"request\" ): a = container . execute_sync ( solved , executor = SyncExecutor ()) assert isinstance ( a , A ) assert B . executed Here B is executed even though A does not depend on it. This is because JoinedDependant leverages the DependantBase interface to tell di that B is a dependency of A even if B is not a parameter or otherwise related to A .","title":"Dependants and the DependantBase"},{"location":"examples/","text":"Examples Simple Example Here is a simple example of how di works: from dataclasses import dataclass from di import Container , Dependant , SyncExecutor class A : ... class B : ... @dataclass class C : a : A b : B def main (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( C , scope = \"request\" )) with container . enter_scope ( \"request\" ): c = container . execute_sync ( solved , executor = SyncExecutor ()) assert isinstance ( c , C ) assert isinstance ( c . a , A ) assert isinstance ( c . b , B ) You will notice that di \"auto-wired\" C : we didn't have to tell it that C depends on A and B , or how to construct A and B , it was all inferred from type annotations. In the wiring and provider registration chapters, you'll see how you can customize this behavior to tell di how to inject things like abstract interfaces or function return values. In-depth example With this background in place, let's dive into a more in-depth example. In this example, we'll look at what it would take for a web framework to provide dependency injection to its users via di . Let's start by looking at the User's code. from di import AsyncExecutor , Container , Dependant # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value async def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): res = await container . execute_async ( solved , values = { Request : Request ( 1 )}, executor = AsyncExecutor () ) assert res == 2 # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value async def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) As a user, you have very little boilerplate. In fact, there is not a single line of code here that is not transmitting information. Now let's look at the web framework side of things. This part can get a bit complex, but it's okay because it's written once, in a library. First, we'll need to create a Container instance. This would be tied to the App or Router instance of the web framework. from di import AsyncExecutor , Container , Dependant # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value async def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): res = await container . execute_async ( solved , values = { Request : Request ( 1 )}, executor = AsyncExecutor () ) assert res == 2 # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value async def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) Next, we \"solve\" the users' endpoint: from di import AsyncExecutor , Container , Dependant # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value async def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): res = await container . execute_async ( solved , values = { Request : Request ( 1 )}, executor = AsyncExecutor () ) assert res == 2 # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value async def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) This should happen once, maybe at app startup. The framework can then store the solved object, which contains all the information necessary to execute the dependency (dependency being in this case the user's endpoint/controller function). This is very important for performance: we want to do the least amount of work possible for each incoming request. Finally, we execute the endpoint for each incoming request: from di import AsyncExecutor , Container , Dependant # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value async def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): res = await container . execute_async ( solved , values = { Request : Request ( 1 )}, executor = AsyncExecutor () ) assert res == 2 # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value async def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) When we do this, we provide the Request instance as a value. This means that di does not introspect at all into the Request to figure out how to build it, it just hands the value off to anything that requests it. You can also directly register providers, which is covered in the provider registration section of the docs. You'll also notice the executor parameter. As you'll see in the [architecture] chapter, one of the fundamental design principles in di is to decouple wiring, solving and execution. This makes it trivial to, for example, enable concurrent execution of dependencies using threads, asynchronous task groups or any other execution paradigm you want.","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#simple-example","text":"Here is a simple example of how di works: from dataclasses import dataclass from di import Container , Dependant , SyncExecutor class A : ... class B : ... @dataclass class C : a : A b : B def main (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( C , scope = \"request\" )) with container . enter_scope ( \"request\" ): c = container . execute_sync ( solved , executor = SyncExecutor ()) assert isinstance ( c , C ) assert isinstance ( c . a , A ) assert isinstance ( c . b , B ) You will notice that di \"auto-wired\" C : we didn't have to tell it that C depends on A and B , or how to construct A and B , it was all inferred from type annotations. In the wiring and provider registration chapters, you'll see how you can customize this behavior to tell di how to inject things like abstract interfaces or function return values.","title":"Simple Example"},{"location":"examples/#in-depth-example","text":"With this background in place, let's dive into a more in-depth example. In this example, we'll look at what it would take for a web framework to provide dependency injection to its users via di . Let's start by looking at the User's code. from di import AsyncExecutor , Container , Dependant # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value async def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): res = await container . execute_async ( solved , values = { Request : Request ( 1 )}, executor = AsyncExecutor () ) assert res == 2 # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value async def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) As a user, you have very little boilerplate. In fact, there is not a single line of code here that is not transmitting information. Now let's look at the web framework side of things. This part can get a bit complex, but it's okay because it's written once, in a library. First, we'll need to create a Container instance. This would be tied to the App or Router instance of the web framework. from di import AsyncExecutor , Container , Dependant # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value async def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): res = await container . execute_async ( solved , values = { Request : Request ( 1 )}, executor = AsyncExecutor () ) assert res == 2 # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value async def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) Next, we \"solve\" the users' endpoint: from di import AsyncExecutor , Container , Dependant # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value async def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): res = await container . execute_async ( solved , values = { Request : Request ( 1 )}, executor = AsyncExecutor () ) assert res == 2 # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value async def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) This should happen once, maybe at app startup. The framework can then store the solved object, which contains all the information necessary to execute the dependency (dependency being in this case the user's endpoint/controller function). This is very important for performance: we want to do the least amount of work possible for each incoming request. Finally, we execute the endpoint for each incoming request: from di import AsyncExecutor , Container , Dependant # Framework code class Request : def __init__ ( self , value : int ) -> None : self . value = value async def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): res = await container . execute_async ( solved , values = { Request : Request ( 1 )}, executor = AsyncExecutor () ) assert res == 2 # User code class MyClass : def __init__ ( self , request : Request ) -> None : self . value = request . value def add ( self , value : int ) -> int : return self . value + value async def controller ( myobj : MyClass ) -> int : return myobj . add ( 1 ) When we do this, we provide the Request instance as a value. This means that di does not introspect at all into the Request to figure out how to build it, it just hands the value off to anything that requests it. You can also directly register providers, which is covered in the provider registration section of the docs. You'll also notice the executor parameter. As you'll see in the [architecture] chapter, one of the fundamental design principles in di is to decouple wiring, solving and execution. This makes it trivial to, for example, enable concurrent execution of dependencies using threads, asynchronous task groups or any other execution paradigm you want.","title":"In-depth example"},{"location":"scopes/","text":"Scopes Scopes are one of the fundamental concepts in dependency injection. Some dependency injection frameworks provide fixes scopes, for example: Singleton: only one instance is created Request: in web frameworks, this could be the lifetime of a request Prototype: re-initialized every time it is needed di generalizes this concept by putting control of scopes into the hands of the users / implementers: a scope in di is identified by any hashable value (a string, enum, int, etc.) and entering / exiting scopes is handled via context managers: async with container . enter_scope ( \"app\" ): async with container . enter_scope ( \"request\" ): async with container . enter_scope ( \"foo, bar, baz!\" ): Scopes provide a framework for several other important features: Dependency lifespans Dependency value sharing Every dependency is linked to a scope. When a scope exits, all dependencies linked to it are destroyed (if they have teardown, the teardown is run) and their value is removed from the cache. This means that dependencies scoped to an outer scope cannot depend on dependencies scoped to an inner scope: from di import Container , Dependant , SyncExecutor from di.typing import Annotated class Request : ... class DBConnection : def __init__ ( self , request : Request ) -> None : ... def controller ( conn : Annotated [ DBConnection , Dependant ( scope = \"app\" )]) -> None : ... def framework () -> None : container = Container ( scopes = ( \"app\" , \"request\" )) with container . enter_scope ( \"app\" ): with container . enter_scope ( \"request\" ): request = Request () with container . register_by_type ( Dependant ( lambda : request , scope = \"request\" ), Request ): container . execute_sync ( container . solve ( Dependant ( controller )), executor = SyncExecutor () ) This example will fail with di.exceptions.ScopeViolationError because an \"app\" scoped dependency ( conn , as requested by controller via Dependant(scope=\"app\") ) depends on a request scope dependency (in framework , we specify Dependant(..., scope=\"request\" ). This is because dependencies and scopes behave much a stack and references in general purpose languages: you can't reference a function local once you exit that function. Even if we could hold onto the value once we exit the scope, that value could be a reference to an object that already had its destructor run, for example a database connection that was closed.","title":"Scopes"},{"location":"scopes/#scopes","text":"Scopes are one of the fundamental concepts in dependency injection. Some dependency injection frameworks provide fixes scopes, for example: Singleton: only one instance is created Request: in web frameworks, this could be the lifetime of a request Prototype: re-initialized every time it is needed di generalizes this concept by putting control of scopes into the hands of the users / implementers: a scope in di is identified by any hashable value (a string, enum, int, etc.) and entering / exiting scopes is handled via context managers: async with container . enter_scope ( \"app\" ): async with container . enter_scope ( \"request\" ): async with container . enter_scope ( \"foo, bar, baz!\" ): Scopes provide a framework for several other important features: Dependency lifespans Dependency value sharing Every dependency is linked to a scope. When a scope exits, all dependencies linked to it are destroyed (if they have teardown, the teardown is run) and their value is removed from the cache. This means that dependencies scoped to an outer scope cannot depend on dependencies scoped to an inner scope: from di import Container , Dependant , SyncExecutor from di.typing import Annotated class Request : ... class DBConnection : def __init__ ( self , request : Request ) -> None : ... def controller ( conn : Annotated [ DBConnection , Dependant ( scope = \"app\" )]) -> None : ... def framework () -> None : container = Container ( scopes = ( \"app\" , \"request\" )) with container . enter_scope ( \"app\" ): with container . enter_scope ( \"request\" ): request = Request () with container . register_by_type ( Dependant ( lambda : request , scope = \"request\" ), Request ): container . execute_sync ( container . solve ( Dependant ( controller )), executor = SyncExecutor () ) This example will fail with di.exceptions.ScopeViolationError because an \"app\" scoped dependency ( conn , as requested by controller via Dependant(scope=\"app\") ) depends on a request scope dependency (in framework , we specify Dependant(..., scope=\"request\" ). This is because dependencies and scopes behave much a stack and references in general purpose languages: you can't reference a function local once you exit that function. Even if we could hold onto the value once we exit the scope, that value could be a reference to an object that already had its destructor run, for example a database connection that was closed.","title":"Scopes"},{"location":"solving/","text":"Solving Solving a dependency means build a directed acyclic graph (DAG) of dependencies by inspecting sub dependencies and resolving binds. Once we solve a dependency, we can execute it without doing any introspection. Solving is done by the Container . The result of solving is stored in a SolvedDependant object which you can pass to Container.execute_{sync,async} to get back the result. The simplest form of executing a dependency is thus: result = container . execute ( container . solve ( Dependant ( lambda : 1 ))) For a more comprehensive overview, see the architecture section. SolvedDependant di lets you pre-solve your dependencies so that you don't have to run the solver each time you execute. This usually comes with a huge performance boost, but only works if you have a static dependency graph. In practice, this just means that solving captures the current binds and won't be updated if there are changes to binds. Note that you can still have values in your DAG change, just not the shape of the DAG itself. For example, here is a more advanced use case where the framework solves the endpoint and then provides the Request as a value each time the endpoint is called. This means that di does not do any reflection for each request, nor does it have to do dependency resolution. from di import Container , Dependant , SyncExecutor from di.api.solved import SolvedDependant # Framework code class Request : ... def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) assert isinstance ( solved , SolvedDependant ) with container . enter_scope ( \"request\" ): container . execute_sync ( solved , values = { Request : Request ()}, executor = SyncExecutor () ) dependencies = solved . get_flat_subdependants () assert all ( isinstance ( item , Dependant ) for item in dependencies ) assert set ( dependant . call for dependant in dependencies ) == { Request , MyClass } # User code class MyClass : ... def controller ( request : Request , myobj : MyClass ) -> None : ... Getting a list of dependencies di provides a convenience function to flatten the dependency DAG into a list off all sub dependencies in Container.get_flat_subdependants . from di import Container , Dependant , SyncExecutor from di.api.solved import SolvedDependant # Framework code class Request : ... def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) assert isinstance ( solved , SolvedDependant ) with container . enter_scope ( \"request\" ): container . execute_sync ( solved , values = { Request : Request ()}, executor = SyncExecutor () ) dependencies = solved . get_flat_subdependants () assert all ( isinstance ( item , Dependant ) for item in dependencies ) assert set ( dependant . call for dependant in dependencies ) == { Request , MyClass } # User code class MyClass : ... def controller ( request : Request , myobj : MyClass ) -> None : ... This lists all of the Dependants for the solved dependency. This means that you can create custom markers and easily enumerate them. For example, you might make a Header dependency and then want to know what headers are being requested by the controller, even if they are nested inside other dependencies: from di import Dependant class Header ( Dependant [ str ]): ... See the dependants section for a more complete example of this.","title":"Solving"},{"location":"solving/#solving","text":"Solving a dependency means build a directed acyclic graph (DAG) of dependencies by inspecting sub dependencies and resolving binds. Once we solve a dependency, we can execute it without doing any introspection. Solving is done by the Container . The result of solving is stored in a SolvedDependant object which you can pass to Container.execute_{sync,async} to get back the result. The simplest form of executing a dependency is thus: result = container . execute ( container . solve ( Dependant ( lambda : 1 ))) For a more comprehensive overview, see the architecture section.","title":"Solving"},{"location":"solving/#solveddependant","text":"di lets you pre-solve your dependencies so that you don't have to run the solver each time you execute. This usually comes with a huge performance boost, but only works if you have a static dependency graph. In practice, this just means that solving captures the current binds and won't be updated if there are changes to binds. Note that you can still have values in your DAG change, just not the shape of the DAG itself. For example, here is a more advanced use case where the framework solves the endpoint and then provides the Request as a value each time the endpoint is called. This means that di does not do any reflection for each request, nor does it have to do dependency resolution. from di import Container , Dependant , SyncExecutor from di.api.solved import SolvedDependant # Framework code class Request : ... def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) assert isinstance ( solved , SolvedDependant ) with container . enter_scope ( \"request\" ): container . execute_sync ( solved , values = { Request : Request ()}, executor = SyncExecutor () ) dependencies = solved . get_flat_subdependants () assert all ( isinstance ( item , Dependant ) for item in dependencies ) assert set ( dependant . call for dependant in dependencies ) == { Request , MyClass } # User code class MyClass : ... def controller ( request : Request , myobj : MyClass ) -> None : ...","title":"SolvedDependant"},{"location":"solving/#getting-a-list-of-dependencies","text":"di provides a convenience function to flatten the dependency DAG into a list off all sub dependencies in Container.get_flat_subdependants . from di import Container , Dependant , SyncExecutor from di.api.solved import SolvedDependant # Framework code class Request : ... def web_framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) assert isinstance ( solved , SolvedDependant ) with container . enter_scope ( \"request\" ): container . execute_sync ( solved , values = { Request : Request ()}, executor = SyncExecutor () ) dependencies = solved . get_flat_subdependants () assert all ( isinstance ( item , Dependant ) for item in dependencies ) assert set ( dependant . call for dependant in dependencies ) == { Request , MyClass } # User code class MyClass : ... def controller ( request : Request , myobj : MyClass ) -> None : ... This lists all of the Dependants for the solved dependency. This means that you can create custom markers and easily enumerate them. For example, you might make a Header dependency and then want to know what headers are being requested by the controller, even if they are nested inside other dependencies: from di import Dependant class Header ( Dependant [ str ]): ... See the dependants section for a more complete example of this.","title":"Getting a list of dependencies"},{"location":"wiring/","text":"Wiring Wiring is the act of \"connecting\" together dependencies. There are generally two types of wiring that a DI container can do: Auto-wiring: where the container inspects the dependencies and automatically deduces their sub-dependencies. Manual wiring: where the user needs to register each sub-dependency with the container. Auto-wiring is generally preferable: it reduces boilerplate and decouples your application from the Container's API. But auto-wiring is not always possible: sometimes the value is produced by a function ( value: int = some_function() ) or the type to inject is not the type in the annotation (when using interfaces / protocols). Auto-wiring in di Auto-wiring in di relies on inspecting function signatures and class constructors. The primary means of inspection are the standard library's inspect.signature and typing.get_type_hints . This makes auto-wiring compatible with a broad range of things, including: def functions Classes functools.partial binds Callable class classes or class instances (classes implementing __call__ ) Here is an example showing auto-wiring in action. Auto-wiring can work with dataclasses, even ones with a default_factory . In this example we'll load a config from the environment: import os from dataclasses import dataclass , field from di import AsyncExecutor , Container , Dependant @dataclass class Config : host : str = field ( default_factory = lambda : os . getenv ( \"HOST\" , \"localhost\" )) class DBConn : def __init__ ( self , config : Config ) -> None : self . host = config . host async def controller ( conn : DBConn ) -> None : assert isinstance ( conn , DBConn ) async def framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): await container . execute_async ( solved , executor = AsyncExecutor ()) What makes this \"auto-wiring\" is that we didn't have to tell di how to construct DBConn : di detected that controller needed a DBConn and that DBConn in turn needs a Config instance. Manual wiring But what about situations where auto-wiring doesn't cut it? A common scenario for this is when type annotations are interfaces / protocols / ABCs, not concrete implementations. This is a good general practice and is very common in larger projects. It is also common for a dependency to come from a function, in which case we don't just want an instance of the type annotation, we want the value returned by a specific function. In these scenarios, some manual input from the user is required. There are two important concepts in di to handle this input: Binds: are used to swap out one dependency for another, which can be used to swap out an interface / protocol / ABC for a concrete implementation. Markers: usually Dependant(...) which tell di how to construct the dependency (e.g. calling a function) as well as carrying other metadata (like the scope, which you will see more about later on). Here is an example that makes use of both: import os import sys from dataclasses import dataclass , field if sys . version_info < ( 3 , 9 ): from typing_extensions import Annotated else : from typing import Annotated from di import AsyncExecutor , Container , Dependant class AbstractDBConn : def execute ( self , query : str ) -> str : ... @dataclass class Config : host : str = field ( default_factory = lambda : os . getenv ( \"HOST\" , \"localhost\" )) class ConcreteDBConn : def __init__ ( self , config : Config ) -> None : self . config = config def execute ( self , query : str ) -> str : return f \"executed { query } \" def get_user ( db : AbstractDBConn ) -> str : # this is a nonsensical query for demonstration purposes # you'd normally want to get the id from the request # and returna User object or something like that return db . execute ( \"SELECT name from Users LIMIT 1\" ) async def controller ( # markers can be added via Annotated user1 : Annotated [ str , Dependant ( get_user , scope = \"request\" )], # or as the default value, in which case types can be checked by MyPy/Pylance user2 : Annotated [ str , Dependant ( get_user , scope = \"request\" )], ) -> None : assert user1 == user2 == \"executed SELECT name from Users LIMIT 1\" async def framework (): container = Container ( scopes = [ \"request\" ]) # note that di will also autowire the bind, in this case to inject Config container . register_by_type ( Dependant ( ConcreteDBConn , scope = \"request\" ), AbstractDBConn ) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): await container . execute_async ( solved , executor = AsyncExecutor ()) Binds in di are particularly powerful because the bound providers can themselves have dependencies, and those dependencies can even be auto-wired. For more information on binds in di , see our Binds docs. Markers are set via PEP 593's Annotated . This is in contrast to FastAPIs use of markers as default values ( param: int = Depends(...) ). When FastAPI was designed, PEP 593 did not exist, and there are several advantages to using PEP 593's Annotated: Compatible with other uses of default values, like dataclass' field or Pydantic's Field . Non-invasive modification of signatures: adding Depends(...) in Annotated should be ignored by anything except di . Functions/classes can be called as normal outside of di and the default values (when present) will be used. Multiple markers can be used. For example, something like Annotated[T, SyncToThread(), Depends()] is possible, or even Annotated[Annotated[T, Dependant()], SyncToThread()] (which is equivalent). With the aliases Provide = Annotated[T, Depends()] and InThread = Annotated[T, SyncToThread()] one can write Provide[InThread[SomeClass]] . There are however some cons to the use of Annotated : Annotated requires Python 3.9 (although it is available via the typing_extensions backport ) Using Annotated is more verbose, and can easily cause your function signature to spill into multiple lines. Performance Reflection (inspecting function signatures for dependencies) is slow. For this reason, di tries to avoid it as much as possible. The best way to avoid extra introspection is to re-use Solved Dependants .","title":"Wiring"},{"location":"wiring/#wiring","text":"Wiring is the act of \"connecting\" together dependencies. There are generally two types of wiring that a DI container can do: Auto-wiring: where the container inspects the dependencies and automatically deduces their sub-dependencies. Manual wiring: where the user needs to register each sub-dependency with the container. Auto-wiring is generally preferable: it reduces boilerplate and decouples your application from the Container's API. But auto-wiring is not always possible: sometimes the value is produced by a function ( value: int = some_function() ) or the type to inject is not the type in the annotation (when using interfaces / protocols).","title":"Wiring"},{"location":"wiring/#auto-wiring-in-di","text":"Auto-wiring in di relies on inspecting function signatures and class constructors. The primary means of inspection are the standard library's inspect.signature and typing.get_type_hints . This makes auto-wiring compatible with a broad range of things, including: def functions Classes functools.partial binds Callable class classes or class instances (classes implementing __call__ ) Here is an example showing auto-wiring in action. Auto-wiring can work with dataclasses, even ones with a default_factory . In this example we'll load a config from the environment: import os from dataclasses import dataclass , field from di import AsyncExecutor , Container , Dependant @dataclass class Config : host : str = field ( default_factory = lambda : os . getenv ( \"HOST\" , \"localhost\" )) class DBConn : def __init__ ( self , config : Config ) -> None : self . host = config . host async def controller ( conn : DBConn ) -> None : assert isinstance ( conn , DBConn ) async def framework (): container = Container ( scopes = [ \"request\" ]) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): await container . execute_async ( solved , executor = AsyncExecutor ()) What makes this \"auto-wiring\" is that we didn't have to tell di how to construct DBConn : di detected that controller needed a DBConn and that DBConn in turn needs a Config instance.","title":"Auto-wiring in di"},{"location":"wiring/#manual-wiring","text":"But what about situations where auto-wiring doesn't cut it? A common scenario for this is when type annotations are interfaces / protocols / ABCs, not concrete implementations. This is a good general practice and is very common in larger projects. It is also common for a dependency to come from a function, in which case we don't just want an instance of the type annotation, we want the value returned by a specific function. In these scenarios, some manual input from the user is required. There are two important concepts in di to handle this input: Binds: are used to swap out one dependency for another, which can be used to swap out an interface / protocol / ABC for a concrete implementation. Markers: usually Dependant(...) which tell di how to construct the dependency (e.g. calling a function) as well as carrying other metadata (like the scope, which you will see more about later on). Here is an example that makes use of both: import os import sys from dataclasses import dataclass , field if sys . version_info < ( 3 , 9 ): from typing_extensions import Annotated else : from typing import Annotated from di import AsyncExecutor , Container , Dependant class AbstractDBConn : def execute ( self , query : str ) -> str : ... @dataclass class Config : host : str = field ( default_factory = lambda : os . getenv ( \"HOST\" , \"localhost\" )) class ConcreteDBConn : def __init__ ( self , config : Config ) -> None : self . config = config def execute ( self , query : str ) -> str : return f \"executed { query } \" def get_user ( db : AbstractDBConn ) -> str : # this is a nonsensical query for demonstration purposes # you'd normally want to get the id from the request # and returna User object or something like that return db . execute ( \"SELECT name from Users LIMIT 1\" ) async def controller ( # markers can be added via Annotated user1 : Annotated [ str , Dependant ( get_user , scope = \"request\" )], # or as the default value, in which case types can be checked by MyPy/Pylance user2 : Annotated [ str , Dependant ( get_user , scope = \"request\" )], ) -> None : assert user1 == user2 == \"executed SELECT name from Users LIMIT 1\" async def framework (): container = Container ( scopes = [ \"request\" ]) # note that di will also autowire the bind, in this case to inject Config container . register_by_type ( Dependant ( ConcreteDBConn , scope = \"request\" ), AbstractDBConn ) solved = container . solve ( Dependant ( controller , scope = \"request\" )) async with container . enter_scope ( \"request\" ): await container . execute_async ( solved , executor = AsyncExecutor ()) Binds in di are particularly powerful because the bound providers can themselves have dependencies, and those dependencies can even be auto-wired. For more information on binds in di , see our Binds docs. Markers are set via PEP 593's Annotated . This is in contrast to FastAPIs use of markers as default values ( param: int = Depends(...) ). When FastAPI was designed, PEP 593 did not exist, and there are several advantages to using PEP 593's Annotated: Compatible with other uses of default values, like dataclass' field or Pydantic's Field . Non-invasive modification of signatures: adding Depends(...) in Annotated should be ignored by anything except di . Functions/classes can be called as normal outside of di and the default values (when present) will be used. Multiple markers can be used. For example, something like Annotated[T, SyncToThread(), Depends()] is possible, or even Annotated[Annotated[T, Dependant()], SyncToThread()] (which is equivalent). With the aliases Provide = Annotated[T, Depends()] and InThread = Annotated[T, SyncToThread()] one can write Provide[InThread[SomeClass]] . There are however some cons to the use of Annotated : Annotated requires Python 3.9 (although it is available via the typing_extensions backport ) Using Annotated is more verbose, and can easily cause your function signature to spill into multiple lines.","title":"Manual wiring"},{"location":"wiring/#performance","text":"Reflection (inspecting function signatures for dependencies) is slow. For this reason, di tries to avoid it as much as possible. The best way to avoid extra introspection is to re-use Solved Dependants .","title":"Performance"}]}